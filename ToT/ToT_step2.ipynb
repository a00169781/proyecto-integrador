{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell if you are using google colab\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fb0FVcge2lt",
        "outputId": "22425ca0-1fe4-4ee4-aaa4-c02c33c9892f"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "# Uncomment the following line to get the API key from the user data\n",
        "# if you run this in colab\n",
        "# from google.colab import userdata\n",
        "# OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# if you run this in local\n",
        "import os\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAPz0G-RzYK1",
        "outputId": "f325dd9d-fe5c-41c9-eae9-910454b4a710"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def get_current_timestamp():\n",
        "  \"\"\"\n",
        "  Returns the current timestamp in the format \"YYYY-MM-DD hh:mm:ss\".\n",
        "  \"\"\"\n",
        "  now = datetime.now()\n",
        "  return now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NB9B9U3RQ8yz",
        "outputId": "a7f31a2f-a4a5-494d-de48-7aa1d0f8f29d"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def get_current_date():\n",
        "  \"\"\"\n",
        "  Returns the current date in the format \"YYYY-MM-DD\".\n",
        "  \"\"\"\n",
        "  now = datetime.now()\n",
        "  return now.strftime(\"%Y-%m-%d\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dGtbOrNQvJ7G"
      },
      "outputs": [],
      "source": [
        "# Uncomment the following line if you are using google colab\n",
        "# from google.colab import drive\n",
        "\n",
        "import json\n",
        "\n",
        "def save_dict_to_drive(data, filename):\n",
        "  \"\"\"Saves a dictionary to a JSON file in Google Drive.\n",
        "\n",
        "  Args:\n",
        "    data: The dictionary to save.\n",
        "    filename: The name of the file to save to (e.g., 'my_data.json').\n",
        "  \"\"\"\n",
        "  # Uncomment the following lines if you are using google colab\n",
        "  # drive.mount('/content/drive')\n",
        "  # filepath = f'/content/drive/MyDrive/Colab/{filename}'\n",
        "\n",
        "  # If you are not using google colab, you can save the file in the current directory\n",
        "  filepath = f'./output/{filename}'\n",
        "  with open(filepath, 'w') as f:\n",
        "    json.dump(data, f, indent=4)\n",
        "  print(f\"Dictionary saved to: {filepath}\")\n",
        "\n",
        "def load_dict_from_drive(filename):\n",
        "  \"\"\"Loads a dictionary from a JSON file in Google Drive.\n",
        "\n",
        "  Args:\n",
        "    filename: The name of the file to load from (e.g., 'my_data.json').\n",
        "\n",
        "  Returns:\n",
        "    The loaded dictionary, or None if the file is not found.\n",
        "  \"\"\"\n",
        "  # Uncomment the following lines if you are using google colab\n",
        "  # drive.mount('/content/drive')\n",
        "  # filepath = f'/content/drive/MyDrive/Colab/{filename}'\n",
        "\n",
        "  # If you are not using google colab, you can load the file from the current directory\n",
        "  filepath = f'./output/{filename}'\n",
        "  try:\n",
        "    with open(filepath, 'r') as f:\n",
        "      data = json.load(f)\n",
        "    print(f\"Dictionary loaded from: {filepath}\")\n",
        "    return data\n",
        "  except FileNotFoundError:\n",
        "    print(f\"File not found: {filepath}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# my_dict = {'key1': 'value1', 'key2': 'value2'}\n",
        "# save_dict_to_drive(my_dict, 'my_dictionary.json')\n",
        "\n",
        "# loaded_dict = load_dict_from_drive('my_dictionary.json')\n",
        "# if loaded_dict:\n",
        "#   print(loaded_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "np7EKiiVvoIg"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Replace with the actual URL of your JSON file on GitHub\n",
        "github_url = \"https://raw.githubusercontent.com/google/BIG-bench/refs/heads/main/bigbench/benchmark_tasks/causal_judgment/task.json\"\n",
        "\n",
        "try:\n",
        "  response = requests.get(github_url)\n",
        "  response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "  data = json.loads(response.text)\n",
        "\n",
        "  # Now you can work with the parsed JSON data\n",
        "  # print(data)\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "  print(f\"Error downloading or parsing JSON: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DwIbPn7Cr4yQ"
      },
      "outputs": [],
      "source": [
        "llm_methods = {\n",
        "  \"zero_shot\": {\n",
        "      \"prompt\": \"\"\"\n",
        "You are an AI language model specialized in determining causal relationships. When presented with a question about causality, analyze the information carefully and provide a clear, concise response. Your answer should:\n",
        "\n",
        "Reasoning: Begin with a brief explanation of whether and how the cause leads to the effect.\n",
        "Conclusion: End with \"The answer is Yes.\" or \"The answer is No.\" based on your analysis.\n",
        "\"\"\" ,\n",
        "      \"file_name\": \"tot_step2\"\n",
        "  },\n",
        "  \"cot\": {\n",
        "      \"prompt\": \"\"\"\n",
        "You are an AI language model tasked with answering questions about causation from the perspective of a typical person. When presented with a causation question, please follow these steps:\n",
        "\n",
        "1. **Understand the Question:**\n",
        "   - Identify the proposed cause and effect in the question.\n",
        "   - Determine what is being asked and any common assumptions involved.\n",
        "\n",
        "2. **Typical Person's Reasoning:**\n",
        "   - Consider how an average person might perceive the relationship between the cause and effect.\n",
        "   - Use everyday knowledge, beliefs, and intuitive reasoning that a typical person might apply.\n",
        "   - Provide a brief explanation reflecting this common-sense reasoning.\n",
        "\n",
        "3. **Conclusion:**\n",
        "   - Summarize your reasoning and state whether the typical person would believe the cause leads to the effect.\n",
        "   - End your response with \"The answer is Yes.\" or \"The answer is No.\" based on this perspective.\n",
        "\n",
        "      \"\"\",\n",
        "      \"file_name\": \"cot\"\n",
        "\n",
        "  },\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the method to use\n",
        "method = \"zero_shot\"\n",
        "llm_params = llm_methods[method]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfpYZnFRmUSr",
        "outputId": "f1cf9d2a-125d-4218-f348-415a45cd8fdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'role': 'system', 'content': '\\nYou are an AI language model specialized in determining causal relationships. When presented with a question about causality, analyze the information carefully and provide a clear, concise response. Your answer should:\\n\\nReasoning: Begin with a brief explanation of whether and how the cause leads to the effect.\\nConclusion: End with \"The answer is Yes.\" or \"The answer is No.\" based on your analysis.\\n'}\n"
          ]
        }
      ],
      "source": [
        "model_params = {\n",
        "    \"model\": \"gpt-4o-mini\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": llm_params['prompt']},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"example['input']\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(model_params['messages'][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HgTuOZyPxJVg"
      },
      "outputs": [],
      "source": [
        "llm_output = {\n",
        "    \"description\": \"Answers of the model for the set of causal questions\",\n",
        "    \"url\": \"https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/causal_judgment/task.json\",\n",
        "    \"model_params\": model_params,\n",
        "    \"timretamp\": get_current_timestamp(),\n",
        "    \"results\" : []\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dictionary loaded from: ./output/llm_output_tot_step1_2024-10-15.json\n"
          ]
        }
      ],
      "source": [
        "data = load_dict_from_drive('llm_output_tot_step1_2024-10-15.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baqGldtw26Br",
        "outputId": "21629079-7cf6-417e-e2ee-ae9ff77f8028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing example 1/190\n",
            "Processing example 2/190\n",
            "Processing example 3/190\n",
            "Processing example 4/190\n",
            "Processing example 5/190\n",
            "Processing example 6/190\n",
            "Processing example 7/190\n",
            "Processing example 8/190\n",
            "Processing example 9/190\n",
            "Processing example 10/190\n",
            "Processing example 11/190\n",
            "Processing example 12/190\n",
            "Processing example 13/190\n",
            "Processing example 14/190\n",
            "Processing example 15/190\n",
            "Processing example 16/190\n",
            "Processing example 17/190\n",
            "Processing example 18/190\n",
            "Processing example 19/190\n",
            "Processing example 20/190\n",
            "Processing example 21/190\n",
            "Processing example 22/190\n",
            "Processing example 23/190\n",
            "Processing example 24/190\n",
            "Processing example 25/190\n",
            "Processing example 26/190\n",
            "Processing example 27/190\n",
            "Processing example 28/190\n",
            "Processing example 29/190\n",
            "Processing example 30/190\n",
            "Processing example 31/190\n",
            "Processing example 32/190\n",
            "Processing example 33/190\n",
            "Processing example 34/190\n",
            "Processing example 35/190\n",
            "Processing example 36/190\n",
            "Processing example 37/190\n",
            "Processing example 38/190\n",
            "Processing example 39/190\n",
            "Processing example 40/190\n",
            "Processing example 41/190\n",
            "Processing example 42/190\n",
            "Processing example 43/190\n",
            "Processing example 44/190\n",
            "Processing example 45/190\n",
            "Processing example 46/190\n",
            "Processing example 47/190\n",
            "Processing example 48/190\n",
            "Processing example 49/190\n",
            "Processing example 50/190\n",
            "Processing example 51/190\n",
            "Processing example 52/190\n",
            "Processing example 53/190\n",
            "Processing example 54/190\n",
            "Processing example 55/190\n",
            "Processing example 56/190\n",
            "Processing example 57/190\n",
            "Processing example 58/190\n",
            "Processing example 59/190\n",
            "Processing example 60/190\n",
            "Processing example 61/190\n",
            "Processing example 62/190\n",
            "Processing example 63/190\n",
            "Processing example 64/190\n",
            "Processing example 65/190\n",
            "Processing example 66/190\n",
            "Processing example 67/190\n",
            "Processing example 68/190\n",
            "Processing example 69/190\n",
            "Processing example 70/190\n",
            "Processing example 71/190\n",
            "Processing example 72/190\n",
            "Processing example 73/190\n",
            "Processing example 74/190\n",
            "Processing example 75/190\n",
            "Processing example 76/190\n",
            "Processing example 77/190\n",
            "Processing example 78/190\n",
            "Processing example 79/190\n",
            "Processing example 80/190\n",
            "Processing example 81/190\n",
            "Processing example 82/190\n",
            "Processing example 83/190\n",
            "Processing example 84/190\n",
            "Processing example 85/190\n",
            "Processing example 86/190\n",
            "Processing example 87/190\n",
            "Processing example 88/190\n",
            "Processing example 89/190\n",
            "Processing example 90/190\n",
            "Processing example 91/190\n",
            "Processing example 92/190\n",
            "Processing example 93/190\n",
            "Processing example 94/190\n",
            "Processing example 95/190\n",
            "Processing example 96/190\n",
            "Processing example 97/190\n",
            "Processing example 98/190\n",
            "Processing example 99/190\n",
            "Processing example 100/190\n",
            "Processing example 101/190\n",
            "Processing example 102/190\n",
            "Processing example 103/190\n",
            "Processing example 104/190\n",
            "Processing example 105/190\n",
            "Processing example 106/190\n",
            "Processing example 107/190\n",
            "Processing example 108/190\n",
            "Processing example 109/190\n",
            "Processing example 110/190\n",
            "Processing example 111/190\n",
            "Processing example 112/190\n",
            "Processing example 113/190\n",
            "Processing example 114/190\n",
            "Processing example 115/190\n",
            "Processing example 116/190\n",
            "Processing example 117/190\n",
            "Processing example 118/190\n",
            "Processing example 119/190\n",
            "Processing example 120/190\n",
            "Processing example 121/190\n",
            "Processing example 122/190\n",
            "Processing example 123/190\n",
            "Processing example 124/190\n",
            "Processing example 125/190\n",
            "Processing example 126/190\n",
            "Processing example 127/190\n",
            "Processing example 128/190\n",
            "Processing example 129/190\n",
            "Processing example 130/190\n",
            "Processing example 131/190\n",
            "Processing example 132/190\n",
            "Processing example 133/190\n",
            "Processing example 134/190\n",
            "Processing example 135/190\n",
            "Processing example 136/190\n",
            "Processing example 137/190\n",
            "Processing example 138/190\n",
            "Processing example 139/190\n",
            "Processing example 140/190\n",
            "Processing example 141/190\n",
            "Processing example 142/190\n",
            "Processing example 143/190\n",
            "Processing example 144/190\n",
            "Processing example 145/190\n",
            "Processing example 146/190\n",
            "Processing example 147/190\n",
            "Processing example 148/190\n",
            "Processing example 149/190\n",
            "Processing example 150/190\n",
            "Processing example 151/190\n",
            "Processing example 152/190\n",
            "Processing example 153/190\n",
            "Processing example 154/190\n",
            "Processing example 155/190\n",
            "Processing example 156/190\n",
            "Processing example 157/190\n",
            "Processing example 158/190\n",
            "Processing example 159/190\n",
            "Processing example 160/190\n",
            "Processing example 161/190\n",
            "Processing example 162/190\n",
            "Processing example 163/190\n",
            "Processing example 164/190\n",
            "Processing example 165/190\n",
            "Processing example 166/190\n",
            "Processing example 167/190\n",
            "Processing example 168/190\n",
            "Processing example 169/190\n",
            "Processing example 170/190\n",
            "Processing example 171/190\n",
            "Processing example 172/190\n",
            "Processing example 173/190\n",
            "Processing example 174/190\n",
            "Processing example 175/190\n",
            "Processing example 176/190\n",
            "Processing example 177/190\n",
            "Processing example 178/190\n",
            "Processing example 179/190\n",
            "Processing example 180/190\n",
            "Processing example 181/190\n",
            "Processing example 182/190\n",
            "Processing example 183/190\n",
            "Processing example 184/190\n",
            "Processing example 185/190\n",
            "Processing example 186/190\n",
            "Processing example 187/190\n",
            "Processing example 188/190\n",
            "Processing example 189/190\n",
            "Processing example 190/190\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import copy\n",
        "llm_output['results'] = []\n",
        "client = OpenAI(\n",
        "    api_key = OPENAI_API_KEY,\n",
        ")\n",
        "\n",
        "for idx, example in enumerate(data['results']):\n",
        "  print(f\"Processing example {idx+1}/{len(data['results'])}\")\n",
        "  q = example['thoughts'] + \"\\n\\n\" + example['input_question']\n",
        "#  print(example['input'])\n",
        "#  for answer in example['target_scores']:\n",
        "#    if example['target_scores'][answer] == 1:\n",
        "#      print(answer)\n",
        "#  print('---')\n",
        "#  print(example['target_scores'])\n",
        "\n",
        "  completion = client.chat.completions.create(\n",
        "      model=model_params['model'],\n",
        "      messages=[\n",
        "          model_params['messages'][0],\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": example['input']\n",
        "          }\n",
        "      ]\n",
        "  )\n",
        "  response = completion.choices[0].message.content\n",
        "  #print(response)\n",
        "  llm_answer = re.findall(r\"The answer is (Yes|No)\\b\",response, re.IGNORECASE)\n",
        "  llm_answers = {\n",
        "          \"Yes\":0,\n",
        "          \"No\":0\n",
        "        }\n",
        "\n",
        "  for posible_answer in llm_answers:\n",
        "    if posible_answer in llm_answer:\n",
        "      llm_answers[posible_answer] = 1\n",
        "\n",
        "  if len(llm_answer) == 1:\n",
        "    score = example['target_scores'][llm_answer[0]]\n",
        "  else:\n",
        "    score = 0\n",
        "  node = {\n",
        "      \"input\": q,\n",
        "      \"target_scores\": example['target_scores'],\n",
        "      \"llm_full_answer\": response,\n",
        "      \"llm_answer\": llm_answers,\n",
        "      \"score\": score\n",
        "  }\n",
        "  llm_output['results'].append(copy.deepcopy(node))\n",
        "  if idx == 2:\n",
        "    # Uncomment to stop the execution after the first 2 questions\n",
        "     #break\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FWRLG6q8vNs",
        "outputId": "ad72271f-82b2-4584-cd63-13a462a22feb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's check if there are unparsed resonses from the LLM\n",
            "All responses were parsed correctly\n"
          ]
        }
      ],
      "source": [
        "print(\"Let's check if there are unparsed resonses from the LLM\")\n",
        "unparsed_responses = 0\n",
        "for idx, task in enumerate(llm_output['results']):\n",
        "  _cnt = 0\n",
        "  for a in task['llm_answer']:\n",
        "    _cnt += task['llm_answer'][a]\n",
        "  if _cnt != 1:\n",
        "    print(f\"We could not determine the LLM for {_cnt} questions, check this tasks:\")\n",
        "    print(idx, task)\n",
        "    unparsed_responses += 1\n",
        "\n",
        "if unparsed_responses == 0:\n",
        "  print(\"All responses were parsed correctly\")\n",
        "else:\n",
        "  print(f\"We could not determine the LLM for {unparsed_responses} questions\")\n",
        "  print(\"You need to manually check the answers for these questions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEZoNsjt-P6l",
        "outputId": "8dd7b1c6-f3dd-42cc-c5f6-b4c70377acb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The performance of the model zero shot is 69.47%\n"
          ]
        }
      ],
      "source": [
        "# Let's calculate the performance\n",
        "score_cnt = 0\n",
        "\n",
        "for idx, task in enumerate(llm_output['results']):\n",
        "  score_cnt += task['score']\n",
        "\n",
        "print(f\"The performance of the model zero shot is {score_cnt/len(llm_output['results']) * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_MaIoRflDBxV"
      },
      "outputs": [],
      "source": [
        "llm_output['performance'] = score_cnt/len(llm_output['results'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3358ksaDdmp",
        "outputId": "96884f04-af57-48e6-ef9a-0051cd532b0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dictionary saved to: ./output/llm_output_cot_2024-10-14.json\n"
          ]
        }
      ],
      "source": [
        "import os.path\n",
        "from os import path\n",
        "\n",
        "date = get_current_date()\n",
        "file_name = f\"llm_output_{llm_params['file_name']}_{date}.json\"\n",
        "\n",
        "save_dict_to_drive(llm_output, file_name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
